# Logical Filtering and Analytical Query Processing in ClickHouse

## 1. Introduction

Once all documents have been extracted, cleaned, and reorganized by the language model, the pipeline enters its final and most analytical stage. Up to this point, the system has dealt with raw files, OCR conversions, PDF text extraction, XML decoding, and semantic interpretation through the LLM. The result of these steps is a collection of JSON structures that capture the essential business information contained in each document. However, these JSON objects—as clean as they are—still need to be transformed into a stable format that analysts and applications can query efficiently. This is where the last stage of the pipeline becomes essential.

The goal of this stage is to turn flexible, semi-structured JSON into a reliable analytical dataset. JSON is excellent for capturing the variety and complexity of administrative documents, but it is not directly suitable for large-scale querying or business intelligence. Different documents may contain different fields, similar fields may appear in different shapes, and timestamps or numerical values may not yet follow a uniform format. If these JSON objects were exposed directly to analysts, every query would require repeated parsing, cleaning, and validation. To avoid this, the pipeline converts the JSON layer into a fully structured analytical layer inside ClickHouse.

This entire transformation relies on two key modules. The first module, `clickhouse_io.py`, is responsible for inserting the structured documents into a dedicated ingestion table. Before insertion, the module applies several important operations: it removes debugging fields or internal metadata added during extraction, verifies that the JSON is complete, and checks whether the document already exists to prevent duplicates. This initial cleaning is crucial because the ingestion table acts as the single source of truth for all downstream processing. Each entry stored in this table represents a faithful but minimal version of the document, ready to be reshaped analytically.

The second module, `data_cube.py`, builds everything that comes after ingestion. Its purpose is to interpret the JSON stored in ClickHouse and convert it into structured analytical views. These views mimic the structure of a relational model by turning JSON keys into typed columns. They also apply logical filters to ensure that only consistent and meaningful documents appear in analytical results. For instance, documents missing essential fields—such as dates, company identifiers, or numeric values—are automatically excluded. Beyond filtering, this module organizes the dataset into a classical star schema: a set of dimensions that describe time, companies, and operation types, all connected to a central fact table that holds the document's metrics.

By combining these two modules, the pipeline succeeds in bridging the gap between the flexibility of JSON extraction and the rigor required for analysis. The ingestion layer keeps all documents in a raw but clean form, while the analytical layer reshapes them into structured views ready for dashboards, queries, and business intelligence tasks. This architecture gives the system both robustness and adaptability: new document types can be added without changing the database structure, and analysts always have a consistent and reliable environment to work with.

## 2. Preparing the JSON documents during insertion

Before the documents are inserted into ClickHouse, they pass through a dedicated cleaning and preparation phase designed to ensure that only relevant and standardized information reaches the analytical layer. Even though the language model produces structured JSON, the output still contains a variety of auxiliary elements that are not meant to be stored in the final database. These may include debugging comments added during development, internal notes generated by the LLM, validation flags used to evaluate extraction quality, or temporary metadata produced during earlier steps of the pipeline such as OCR or PDF parsing. All of these extra fields are removed systematically, leaving only the business-relevant content that represents the true information extracted from the document.

Once this pruning is complete, the pipeline performs a deduplication step that ensures no document is stored twice. Because administrative documents can appear multiple times in MinIO, or because the pipeline may be executed repeatedly on the same dataset, relying only on the file name or the timestamp would not be sufficient. Instead, the system builds a canonical version of the JSON: the keys are sorted in a consistent order, unnecessary whitespace is removed, and the structure is normalized so that two equivalent JSONs always produce the same canonical representation. This version is then compared directly with the JSON entries already stored in the ingestion table. If the canonical JSON already exists in ClickHouse, the document is considered a duplicate and is ignored. If not, it is inserted into the table. This strategy guarantees idempotency: running the pipeline multiple times will never insert the same document twice.

After cleaning and deduplication, each valid document is stored as a single row in ClickHouse. The ingestion table acts as a stable archive of the structured documents and contains three types of information: the document type, which indicates whether it comes from a PDF, XML, invoice, receipt, or another category; a timestamp showing when this specific version was ingested; and the full structured JSON, preserved exactly as produced by the language model after cleaning. This table forms the foundation of the analytical system. All downstream transformations, filters, and aggregations rely on this stable and minimal representation of the original document.


## 3. Converting raw JSON into analytical views

The ingestion table keeps the data flexible by storing the documents exactly as JSON. However, analytical work requires structured columns. To bridge this gap, the pipeline uses a set of ClickHouse views defined in `data_cube.py`. These views extract fields from the JSON and convert them into typed columns such as strings, integers, floats, and dates.

This transformation is essential because JSON values may vary in shape, and documents of different types may contain different fields. By using ClickHouse’s JSON extraction functions inside views, the system preserves the flexibility of ingestion while exposing a clean, relational structure for analysis.

## 4. Applying logical filters inside the analytical views

Each analytical view defined in `data_cube.py` contains an essential layer of logical filtering designed to protect the quality of the dataset. Although the ingestion table stores every cleaned JSON document produced by the pipeline, it does not guarantee that all documents are complete, valid, or analytically useful. Some may be missing important fields, others may contain malformed dates, and some might even correspond to document types that the analytical model is not intended to process. To avoid these issues, each view applies a series of filters that ensure only reliable records appear in downstream queries.

### 4.1 Filtering by document type

The first filter checks whether the stored document corresponds to the type of record expected by the analytical view. Since all documents—regardless of type—are stored together in the ingestion table, this filtering step is necessary to avoid mixing unrelated documents. For example, the analytical view dedicated to invoices only accepts entries whose `doc_type` field matches the expected invoice category, such as "danfe". Any document with a different type is immediately excluded. This ensures that each analytical view remains focused on a single, well-defined document structure.

### 4.2 Ensuring the presence of required fields

After verifying the document type, the view checks whether the JSON contains all fields that are essential for analytical queries. These fields vary depending on the document type but typically include identifiers (such as company IDs), timestamps (such as emission or registration dates), and key numerical values (such as invoice totals). If any of these mandatory elements is missing or empty, the record is discarded. This prevents the analytical layer from being polluted by incomplete documents that would otherwise result in misleading visualizations, broken aggregations, or null-value propagation.

### 4.3 Validating field integrity and minimal content

Logical filters also ensure that the document contains a minimum amount of meaningful content before being included in the analytical view. For example, documents whose structured JSON contains only a handful of keys—typically the result of OCR errors or incomplete LLM parsing—are removed. Likewise, JSON structures that exist but have no internal values (such as empty objects or arrays) are excluded. This helps maintain a high signal-to-noise ratio in the analytical model and protects the system from unstable extraction cases.

### 4.4 Standardizing date fields

Administrative documents frequently contain dates in inconsistent formats, such as “12/03/2024”, “2024-03-12”, or even strings like “12 March 2024”. Because these raw values come from the language model, they may also include minor variations depending on how the model interpreted the text. To ensure consistency, each analytical view applies ClickHouse functions to transform textual date representations into proper `Date` or `DateTime` types.  
This standardization is crucial for reliable time-indexed analysis, such as grouping by month, ordering records chronologically, or computing year-over-year comparisons.

### 4.5 Guaranteeing analytical stability

Together, these filters play a vital role in ensuring that the analytical dataset remains stable and meaningful. Instead of exposing everything stored in the ingestion table, the views present only those documents that meet strict validity conditions. This design choice avoids incorrect or misleading results during analysis and ensures that dashboards, aggregations, and automated metrics rely exclusively on trustworthy data. In other words, the logical filters act as a safety net that protects the analytical layer from extraction imperfections, LLM inconsistencies, or unexpected document variations.


## 5. Building the analytical star schema

Once filtering is complete, the analytical layer organizes the data into a star-schema model. This schema is generated entirely from the ingestion table and requires no manual table creation.

A complete time dimension is created covering several years. It contains the breakdown of days, months, quarters, and years, enabling time-series analysis at any level.

A company dimension is created by extracting company identifiers from the JSON and assigning them stable keys using hashing. This dimension includes information related to both senders and recipients of the documents.

Another dimension describes the nature of the operations. The pipeline analyzes the raw field representing the business operation and categorizes it using predefined rules. This allows grouping and comparing the different types of transactions.

Finally, the fact table, created as a ClickHouse view, links all dimensions with the numerical metrics extracted from the documents such as invoice values or reported amounts. This table becomes the central entry point for business analysis.

## 6. Querying the analytical layer

With the analytical views built and the dimensions connected, querying becomes simple and consistent. Analysts no longer need to navigate raw JSON structures or interpret inconsistent fields. Instead, they can rely on stable columns that reflect cleaned, validated, and normalized values.

Operations such as computing totals per month, identifying the most active companies, or analyzing trends across different operation types are made straightforward. The filtering logic applied earlier ensures reliable analytical results.

The ClickHouse layer therefore transforms a heterogeneous set of administrative documents into a unified analytical model ready for dashboards or business intelligence tools.

## 7. Connecting the Analytical Data App to the Data Cube

With the data cube fully constructed in ClickHouse, the final component of the architecture is the analytical application that consumes this structured information. Although the earlier stages of the pipeline are focused on ingestion, extraction, semantic processing, and validation, the goal of the entire process is ultimately to make the information accessible for business exploration. The data-app acts as the interface between the analytical schema produced by the pipeline and the end users who rely on this data for reporting, monitoring, or decision-making.

The connection between the data-app and the analytical layer is made possible thanks to the standardized structure built inside `data_cube.py`. Instead of querying raw JSON, the application communicates exclusively with stable ClickHouse views representing dimensions and fact tables. Because ClickHouse supports high-performance SQL querying, the data-app only needs a SQL client or a lightweight abstraction layer to interact with the data cube. Most data-apps connect using the native HTTP interface exposed by ClickHouse, meaning that each analytical request—from a dashboard, a filter panel, or a graph—corresponds to a live SQL query executed on the underlying views.

This approach offers several advantages. First, the data-app interacts with a model that is fully typed: dates are proper `Date` objects, monetary values are numerical types, and identifiers are normalized. This guarantees stable and predictable behavior, regardless of the complexity of the original documents. Second, because the data cube follows a star-schema structure, the data-app can easily perform cross-dimensional analyses. For instance, a single query can combine the time dimension with the company dimension and the fact table to compute aggregates such as monthly totals per provider or yearly comparisons across activity types.

The views defined in `data_cube.py` also remove any need for the data-app to know how the ingestion table is organized internally. The application is entirely decoupled from the ingestion logic: it does not need to parse JSON, interpret document types, or manage missing fields. All such concerns were handled earlier in the pipeline. This separation of responsibilities dramatically simplifies the frontend layer, which can focus solely on analytical logic and visualization.

In practice, the data-app typically issues SQL queries directly against the fact table view since it aggregates all numerical metrics and connects to the different dimensions through predefined keys. For example, requesting the total revenue per month involves joining the fact table with the time dimension; identifying the most active suppliers requires joining with the company dimension. Because these relationships are already defined inside the analytical views, the data-app only executes straightforward SQL queries without needing to handle low-level data manipulation.

This design also makes the system extensible. If a new type of document is added to the ingestion pipeline, the data cube can be extended by creating a new analytical view for that document type. The data-app does not require any structural change; it simply gains access to new fields and metrics exposed through ClickHouse. The same advantage applies if the company decides to integrate additional dashboards or external BI tools: as long as they can connect to ClickHouse, they can directly read from the data cube.

In summary, the integration between the data cube and the data-app offers a clean and efficient path from complex administrative documents to ready-to-use business insights. The pipeline ensures that the analytical layer is consistent, validated, and fully structured, while the data-app leverages this model to provide fast, reliable, and meaningful access to the underlying information. Together, they form the final bridge between raw document ingestion and actionable, real-world analysis.


## 8. Conclusion

The logical filtering and analytical query layer completes the transformation pipeline. It converts flexible, semi-structured JSON documents into a clean and stable analytical environment. By combining strict validation, dynamic JSON extraction, and a robust star-schema model, the system enables scalable and reliable analysis of incoming documents.

This architecture remains fully extensible. New document types can be added by simply extending the JSON schema and writing new analytical views. The ingestion layer stays unchanged, and the system continues to function without interruption. This approach ensures long-term flexibility while preserving analytical consistency.
